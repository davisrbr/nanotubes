{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, shutil, glob\n",
    "\n",
    "num_validation_file = 1000\n",
    "test_validation_file = 1000\n",
    "File_List = glob.glob('file path_total')\n",
    "list_index = list(np.arange(0, len(File_List), 1))\n",
    "random_index = random.sample(list_index,len(File_List))\n",
    "for i in range(len(random_index)):\n",
    "    random_index_ = random_index[i]\n",
    "    print(random_index_)\n",
    "#==========================================================================VALIDATION SET==========================================\n",
    "    if i < num_validation_file:\n",
    "        shutil.copy('file path_total' %random_index_, 'file path_Validation' %random_index_)\n",
    "#==========================================================================Test SET==========================================\n",
    "    if num_validation_file <= i and num_validation_file + test_validation_file > i:\n",
    "        shutil.copy('file path_total' %random_index_, 'file path_Test' %random_index_)\n",
    "#==========================================================================Training SET==========================================    \n",
    "    if num_validation_file + test_validation_file <= i:\n",
    "        shutil.copy('file path_total' %random_index_, 'file path_Training' %random_index_)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(114,), dtype=float32)\n",
      "Tensor(\"batch:0\", shape=(1000, 4501), dtype=float32) Tensor(\"batch:1\", shape=(1000, 114), dtype=float32) Tensor(\"Reshape:0\", shape=(1000, 3), dtype=float32) Tensor(\"batch:3\", shape=(1000, 1), dtype=float32)\n",
      "Tensor(\"add_3:0\", shape=(114,), dtype=float32)\n",
      "Tensor(\"batch_1:0\", shape=(1000, 4501), dtype=float32) Tensor(\"batch_1:1\", shape=(1000, 114), dtype=float32) Tensor(\"Reshape_1:0\", shape=(1000, 3), dtype=float32) Tensor(\"batch_1:3\", shape=(1000, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_files = 6010\n",
    "num_files_te = 1000\n",
    "vld_num_files = 1000\n",
    "X_length = 4511\n",
    "batch_size_ = 1000\n",
    "n_classes = 38\n",
    "epochs = 1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer([('file path' % i) for i in range(num_files)], \n",
    "                                                    shuffle=True, name='filename_queue')\n",
    "    reader = tf.TextLineReader()\n",
    "    key, value = reader.read(filename_queue)\n",
    "    record_defaults = [[0] for _ in range(X_length)]\n",
    "    record_defaults = [tf.constant([0], dtype=tf.float32) for _ in range(X_length)]\n",
    "    xy_data = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    xy_data = tf.stack(xy_data)\n",
    "   \n",
    "    A_1=tf.cast(xy_data[-4], tf.float32)\n",
    "    A_2=tf.cast(xy_data[-3], tf.float32)\n",
    "    A_3=tf.cast(xy_data[-2], tf.float32)\n",
    "    y_1=tf.cast(xy_data[-7], tf.int32)\n",
    "    y_2=tf.cast(xy_data[-6], tf.int32)\n",
    "    y_3=tf.cast(xy_data[-5], tf.int32)\n",
    "\n",
    "    # 3_Level_Fraction_Prediction(Training)    \n",
    "    y_1 = tf.cond(tf.greater_equal(A_1, 0.6667), lambda:y_1+76, lambda:y_1)\n",
    "    y_1 = tf.cond(tf.greater_equal(A_1, 0.3333) & tf.less(A_1, 0.6667), lambda:y_1+38, lambda:y_1)\n",
    "    y_2 = tf.cond(tf.greater_equal(A_2, 0.6667), lambda:y_2+76, lambda:y_2)\n",
    "    y_2 = tf.cond(tf.greater_equal(A_2, 0.3333) & tf.less(A_2, 0.6667), lambda:y_2+38, lambda:y_2)\n",
    "    y_3 = tf.cond(tf.greater_equal(A_3, 0.6667), lambda:y_3+76, lambda:y_3)\n",
    "    y_3 = tf.cond(tf.greater_equal(A_3, 0.3333) & tf.less(A_3, 0.6667), lambda:y_3+38, lambda:y_3)\n",
    "    y_1=tf.one_hot(y_1, n_classes*3)\n",
    "    y_2=tf.one_hot(y_2, n_classes*3)\n",
    "    y_3=tf.one_hot(y_3, n_classes*3)\n",
    "    \n",
    "    # 4_Level_Fraction_Prediction(Training)  \n",
    "\"\"\"\n",
    "    y_1 = tf.cond(tf.greater_equal(A_1, 0.75), lambda:y_1+114, lambda:y_1)\n",
    "    y_1 = tf.cond(tf.greater_equal(A_1, 0.5) & tf.less(A_1, 0.75), lambda:y_1+76, lambda:y_1)\n",
    "    y_1 = tf.cond(tf.greater_equal(A_1, 0.25) & tf.less(A_1, 0.5), lambda:y_1+38, lambda:y_1)\n",
    "    y_2 = tf.cond(tf.greater_equal(A_2, 0.75), lambda:y_2+114, lambda:y_2)\n",
    "    y_2 = tf.cond(tf.greater_equal(A_2, 0.5) & tf.less(A_2, 0.75), lambda:y_2+76, lambda:y_2)\n",
    "    y_2 = tf.cond(tf.greater_equal(A_2, 0.25) & tf.less(A_2, 0.5), lambda:y_2+38, lambda:y_2)\n",
    "    y_3 = tf.cond(tf.greater_equal(A_3, 0.75), lambda:y_3+114, lambda:y_3)\n",
    "    y_3 = tf.cond(tf.greater_equal(A_3, 0.5) & tf.less(A_3, 0.75), lambda:y_3+76, lambda:y_3)\n",
    "    y_3 = tf.cond(tf.greater_equal(A_3, 0.25) & tf.less(A_3, 0.5), lambda:y_3+38, lambda:y_3)\n",
    "    y_1=tf.one_hot(y_1, n_classes*4)\n",
    "    y_2=tf.one_hot(y_2, n_classes*4)\n",
    "    y_3=tf.one_hot(y_3, n_classes*4)\n",
    "\"\"\"    \n",
    "    # 5_Level_Fraction_Prediction(Training)\n",
    "\"\"\"    \n",
    "    y_1 = tf.cond(tf.greater_equal(A_1, 0.8), lambda:y_1+152, lambda:y_1)\n",
    "    y_1 = tf.cond(tf.greater_equal(A_1, 0.6) & tf.less(A_1, 0.8), lambda:y_1+114, lambda:y_1)\n",
    "    y_1 = tf.cond(tf.greater_equal(A_1, 0.4) & tf.less(A_1, 0.6), lambda:y_1+76, lambda:y_1)\n",
    "    y_1 = tf.cond(tf.greater_equal(A_1, 0.2) & tf.less(A_1, 0.4), lambda:y_1+38, lambda:y_1)\n",
    "    y_2 = tf.cond(tf.greater_equal(A_2, 0.8), lambda:y_2+152, lambda:y_2)\n",
    "    y_2 = tf.cond(tf.greater_equal(A_2, 0.6) & tf.less(A_2, 0.8), lambda:y_2+114, lambda:y_2)\n",
    "    y_2 = tf.cond(tf.greater_equal(A_2, 0.4) & tf.less(A_2, 0.6), lambda:y_2+76, lambda:y_2)\n",
    "    y_2 = tf.cond(tf.greater_equal(A_2, 0.2) & tf.less(A_2, 0.4), lambda:y_2+38, lambda:y_2)\n",
    "    y_3 = tf.cond(tf.greater_equal(A_3, 0.8), lambda:y_3+152, lambda:y_3)\n",
    "    y_3 = tf.cond(tf.greater_equal(A_3, 0.6) & tf.less(A_3, 0.8), lambda:y_3+114, lambda:y_3)\n",
    "    y_3 = tf.cond(tf.greater_equal(A_3, 0.4) & tf.less(A_3, 0.6), lambda:y_3+76, lambda:y_3)\n",
    "    y_3 = tf.cond(tf.greater_equal(A_3, 0.2) & tf.less(A_3, 0.4), lambda:y_3+38, lambda:y_3)\n",
    "    y_1=tf.one_hot(y_1, n_classes*5)\n",
    "    y_2=tf.one_hot(y_2, n_classes*5)\n",
    "    y_3=tf.one_hot(y_3, n_classes*5)\n",
    "\"\"\"    \n",
    "    y_data = y_1 + y_2 + y_3  \n",
    "    y_data = tf.to_float(y_data)\n",
    "    X_train, y_train,  y_train_p, y_train_ind = tf.train.batch([xy_data[:-10], y_data,  xy_data[-4:-1],\n",
    "                                                                xy_data[-1:]], batch_size = batch_size_)\n",
    "    y_train_p, _ = tf.nn.top_k(y_train_p, k=3, sorted=True)\n",
    "    y_train_p = tf.reshape(y_train_p, [batch_size_, 3])\n",
    "\n",
    "#==========================================================================VALIDATION SET==========================================\n",
    "\n",
    "    filename_queue_vld = tf.train.string_input_producer([('file path' % i) for i in range(vld_num_files)],\n",
    "                                                        shuffle=True, name='filename_queue')\n",
    "    reader_vld = tf.TextLineReader()\n",
    "    key, value_vld = reader.read(filename_queue)\n",
    "    record_defaults = [[0] for _ in range(X_length)]\n",
    "    record_defaults = [tf.constant([0], dtype=tf.float32) for _ in range(X_length)]\n",
    "    xy_data_vld = tf.decode_csv(value_vld, record_defaults=record_defaults)\n",
    "    xy_data_vld = tf.stack(xy_data_vld)\n",
    "      \n",
    "    A_1_vld=tf.cast(xy_data_vld[-4], tf.float32)\n",
    "    A_2_vld=tf.cast(xy_data_vld[-3], tf.float32)\n",
    "    A_3_vld=tf.cast(xy_data_vld[-2], tf.float32)\n",
    "    y_1_vld=tf.cast(xy_data_vld[-7], tf.int32)\n",
    "    y_2_vld=tf.cast(xy_data_vld[-6], tf.int32)\n",
    "    y_3_vld=tf.cast(xy_data_vld[-5], tf.int32)\n",
    "\n",
    "    # 3_Level_Fraction_Prediction(Validation) \n",
    "    y_1_vld = tf.cond(tf.greater_equal(A_1_vld, 0.6667), lambda:y_1_vld+76, lambda:y_1_vld)\n",
    "    y_1_vld = tf.cond(tf.greater_equal(A_1_vld, 0.3333) & tf.less(A_1_vld, 0.6667), lambda:y_1_vld+38, lambda:y_1_vld)\n",
    "    y_2_vld = tf.cond(tf.greater_equal(A_2_vld, 0.6667), lambda:y_2_vld+76, lambda:y_2_vld)\n",
    "    y_2_vld = tf.cond(tf.greater_equal(A_2_vld, 0.3333) & tf.less(A_2_vld, 0.6667), lambda:y_2_vld+38, lambda:y_2_vld)\n",
    "    y_3_vld = tf.cond(tf.greater_equal(A_3_vld, 0.6667), lambda:y_3_vld+76, lambda:y_3_vld)\n",
    "    y_3_vld = tf.cond(tf.greater_equal(A_3_vld, 0.3333) & tf.less(A_3_vld, 0.6667), lambda:y_3_vld+38, lambda:y_3_vld)\n",
    "    y_1_vld=tf.one_hot(y_1_vld, n_classes*3)\n",
    "    y_2_vld=tf.one_hot(y_2_vld, n_classes*3)\n",
    "    y_3_vld=tf.one_hot(y_3_vld, n_classes*3)\n",
    "   \n",
    "    # 4_Level_Fraction_Prediction(Validation)  \n",
    "\"\"\"\n",
    "    y_1_vld = tf.cond(tf.greater_equal(A_1_vld, 0.75), lambda:y_1_vld+114, lambda:y_1_vld)\n",
    "    y_1_vld = tf.cond(tf.greater_equal(A_1_vld, 0.5) & tf.less(A_1_vld, 0.75), lambda:y_1_vld+76, lambda:y_1_vld)\n",
    "    y_1_vld = tf.cond(tf.greater_equal(A_1_vld, 0.25) & tf.less(A_1_vld, 0.5), lambda:y_1_vld+38, lambda:y_1_vld)\n",
    "    y_2_vld = tf.cond(tf.greater_equal(A_2_vld, 0.75), lambda:y_2_vld+114, lambda:y_2_vld)\n",
    "    y_2_vld = tf.cond(tf.greater_equal(A_2_vld, 0.5) & tf.less(A_2_vld, 0.75), lambda:y_2_vld+76, lambda:y_2_vld)\n",
    "    y_2_vld = tf.cond(tf.greater_equal(A_2_vld, 0.25) & tf.less(A_2_vld, 0.5), lambda:y_2_vld+38, lambda:y_2_vld)\n",
    "    y_3_vld = tf.cond(tf.greater_equal(A_3_vld, 0.75), lambda:y_3_vld+114, lambda:y_3_vld)\n",
    "    y_3_vld = tf.cond(tf.greater_equal(A_3_vld, 0.5) & tf.less(A_3_vld, 0.75), lambda:y_3_vld+76, lambda:y_3_vld)\n",
    "    y_3_vld = tf.cond(tf.greater_equal(A_3_vld, 0.25) & tf.less(A_3_vld, 0.5), lambda:y_3_vld+38, lambda:y_3_vld)\n",
    "    y_1_vld=tf.one_hot(y_1_vld, n_classes*4)\n",
    "    y_2_vld=tf.one_hot(y_2_vld, n_classes*4)\n",
    "    y_3_vld=tf.one_hot(y_3_vld, n_classes*4)\n",
    "\"\"\"    \n",
    "    # 5_Level_Fraction_Prediction(Validation)\n",
    "\"\"\"    \n",
    "    y_1_vld = tf.cond(tf.greater_equal(A_1_vld, 0.8), lambda:y_1_vld+152, lambda:y_1_vld)\n",
    "    y_1_vld = tf.cond(tf.greater_equal(A_1_vld, 0.6) & tf.less(A_1_vld, 0.8), lambda:y_1_vld+114, lambda:y_1_vld)\n",
    "    y_1_vld = tf.cond(tf.greater_equal(A_1_vld, 0.4) & tf.less(A_1_vld, 0.6), lambda:y_1_vld+76, lambda:y_1_vld)\n",
    "    y_1_vld = tf.cond(tf.greater_equal(A_1_vld, 0.2) & tf.less(A_1_vld, 0.4), lambda:y_1_vld+38, lambda:y_1_vld)\n",
    "    y_2_vld = tf.cond(tf.greater_equal(A_2_vld, 0.8), lambda:y_2_vld+152, lambda:y_2_vld)\n",
    "    y_2_vld = tf.cond(tf.greater_equal(A_2_vld, 0.6) & tf.less(A_2_vld, 0.8), lambda:y_2_vld+114, lambda:y_2_vld)\n",
    "    y_2_vld = tf.cond(tf.greater_equal(A_2_vld, 0.4) & tf.less(A_2_vld, 0.6), lambda:y_2_vld+76, lambda:y_2_vld)\n",
    "    y_2_vld = tf.cond(tf.greater_equal(A_2_vld, 0.2) & tf.less(A_2_vld, 0.4), lambda:y_2_vld+38, lambda:y_2_vld)\n",
    "    y_3_vld = tf.cond(tf.greater_equal(A_3_vld, 0.8), lambda:y_3_vld+152, lambda:y_3_vld)\n",
    "    y_3_vld = tf.cond(tf.greater_equal(A_3_vld, 0.6) & tf.less(A_3_vld, 0.8), lambda:y_3_vld+114, lambda:y_3_vld)\n",
    "    y_3_vld = tf.cond(tf.greater_equal(A_3_vld, 0.4) & tf.less(A_3_vld, 0.6), lambda:y_3_vld+76, lambda:y_3_vld)\n",
    "    y_3_vld = tf.cond(tf.greater_equal(A_3_vld, 0.2) & tf.less(A_3_vld, 0.4), lambda:y_3_vld+38, lambda:y_3_vld)\n",
    "    y_1_vld=tf.one_hot(y_1_vld, n_classes*5)\n",
    "    y_2_vld=tf.one_hot(y_2_vld, n_classes*5)\n",
    "    y_3_vld=tf.one_hot(y_3_vld, n_classes*5)\n",
    "\"\"\"    \n",
    "    y_data_vld = y_1_vld + y_2_vld + y_3_vld\n",
    "    y_data_vld = tf.to_float(y_data_vld)\n",
    "    X_vld, y_vld, y_vld_p, y_vld_ind = tf.train.batch([xy_data_vld[:-10], y_data_vld, xy_data_vld[-4:-1], \n",
    "                                                       xy_data_vld[-1:]], batch_size = batch_size_)\n",
    "    y_vld_p, _ = tf.nn.top_k(y_vld_p, k=3, sorted=True)\n",
    "    y_vld_p = tf.reshape(y_vld_p, [batch_size_, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1126, 64)\n",
      "(?, 126, 64)\n",
      "(?, 8064)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    inputs_ = tf.placeholder(tf.float32, [None, 4501, 1], name = 'inputs')\n",
    "    labels_1 = tf.placeholder(tf.float32, [None, n_classes*3], name = 'labels_1')\n",
    "    labels_2 = tf.placeholder(tf.float32, [None, 3], name = 'labels_2')\n",
    "    logit_num = tf.placeholder(tf.int32, [None, 3], name = 'logits_Top_3')\n",
    "    label_num = tf.placeholder(tf.int32, [None, 3], name = 'labels_Top_3')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "\n",
    "    #   model architecture(CNN_2F)        \n",
    "\"\"\"\"\" \n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=64, kernel_size=50, strides=2,padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=2, padding='same')\n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=25, strides=3, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=3, padding='same')\n",
    "    flat = tf.reshape(max_pool_2, (-1, 126*64))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    logits = tf.layers.dense(flat, 2000, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits= tf.nn.dropout(logits, keep_prob=keep_prob_)\n",
    "    logits_ = tf.layers.dense(logits, 500, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits_= tf.nn.dropout(logits_, keep_prob=keep_prob_)\n",
    "    logits_1 = tf.layers.dense(logits_, n_classes*3, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\"\"\"\n",
    "    #   model architecture(CNN_3F)\n",
    "\"\"\"    \n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=64, kernel_size=20, strides=1,padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=3, padding='same')\n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=15, strides=1, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=3, padding='same')\n",
    "    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=64, kernel_size=10, strides=2, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=1, strides=2, padding='same')\n",
    "    flat = tf.reshape(max_pool_3, (-1, 126*64))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    logits = tf.layers.dense(flat, 2500, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits= tf.nn.dropout(logits, keep_prob=keep_prob_)\n",
    "    logits_ = tf.layers.dense(logits, 1000, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits_= tf.nn.dropout(logits_, keep_prob=keep_prob_)\n",
    "    logits_1 = tf.layers.dense(logits_, n_classes*3, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\"\"\"\n",
    "    #   model architecture(CNN_4F)    \n",
    "\"\"\"\n",
    "    conv = tf.layers.conv1d(inputs=inputs_, filters=64, kernel_size=25, strides=1,padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool = tf.layers.max_pooling1d(inputs=conv, pool_size=3, strides=2, padding='same')\n",
    "    conv1 = tf.layers.conv1d(inputs=max_pool, filters=64, kernel_size=20, strides=1,padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=2, padding='same')\n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=15, strides=1, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same')\n",
    "    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=64, kernel_size=10, strides=2, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=1, strides=2, padding='same')\n",
    "    flat = tf.reshape(max_pool_3, (-1, 141*64))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    logits = tf.layers.dense(flat, 2500, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits= tf.nn.dropout(logits, keep_prob=keep_prob_)\n",
    "    logits_ = tf.layers.dense(logits, 1000, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits_= tf.nn.dropout(logits_, keep_prob=keep_prob_)\n",
    "    logits_1 = tf.layers.dense(logits_, n_classes*3, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\"\"\" \n",
    "    #   model architecture(CNN_5F)      \n",
    "\n",
    "    conv = tf.layers.conv1d(inputs=inputs_, filters=64, kernel_size=30, strides=1,padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool = tf.layers.max_pooling1d(inputs=conv, pool_size=3, strides=2, padding='same')\n",
    "    conv1 = tf.layers.conv1d(inputs=max_pool, filters=64, kernel_size=25, strides=1,padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=2, padding='same')\n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=20, strides=1, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same')\n",
    "    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=64, kernel_size=15, strides=1, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=1, strides=2, padding='same')\n",
    "    conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=64, kernel_size=10, strides=1, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=1, strides=2, padding='same')\n",
    "    flat = tf.reshape(max_pool_4, (-1, 141*64))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    logits = tf.layers.dense(flat, 2500, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits= tf.nn.dropout(logits, keep_prob=keep_prob_)\n",
    "    logits_ = tf.layers.dense(logits, 1000, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits_= tf.nn.dropout(logits_, keep_prob=keep_prob_)\n",
    "    logits_1 = tf.layers.dense(logits_, n_classes*3, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    " \n",
    "    #   model architecture(CNN_6F) \n",
    "\"\"\"    \n",
    "    conv = tf.layers.conv1d(inputs=inputs_, filters=64, kernel_size=35, strides=1,padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool = tf.layers.max_pooling1d(inputs=conv, pool_size=3, strides=2, padding='same')\n",
    "    conv1 = tf.layers.conv1d(inputs=max_pool, filters=64, kernel_size=30, strides=1,padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=2, padding='same')\n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=25, strides=1, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same')\n",
    "    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=64, kernel_size=20, strides=1, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=1, strides=2, padding='same')\n",
    "    conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=64, kernel_size=15, strides=1, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=1, strides=2, padding='same')\n",
    "    conv5 = tf.layers.conv1d(inputs=max_pool_4, filters=64, kernel_size=10, strides=1, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_5 = tf.layers.max_pooling1d(inputs=conv4, pool_size=1, strides=2, padding='same')\n",
    "    flat = tf.reshape(max_pool_5, (-1, 71*64))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    logits = tf.layers.dense(flat, 2500, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits= tf.nn.dropout(logits, keep_prob=keep_prob_)\n",
    "    logits_ = tf.layers.dense(logits, 1000, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits_= tf.nn.dropout(logits_, keep_prob=keep_prob_)\n",
    "    logits_1 = tf.layers.dense(logits_, n_classes*3, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\"\"\"\n",
    "    #   model architecture(CNN_3I)\n",
    "\"\"\" \n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=64, kernel_size=20, strides=2,padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=3, padding='same')\n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=15, strides=1, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=3, padding='same')\n",
    "    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=64, kernel_size=10, strides=1, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=1, strides=2, padding='same')\n",
    "    conv1_11 = tf.layers.conv1d(inputs=max_pool_3, filters=22, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv1_21 = tf.layers.conv1d(inputs=max_pool_3, filters=32, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv1_31 = tf.layers.conv1d(inputs=max_pool_3, filters=6, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    avg_pool1_41 = tf.layers.average_pooling1d(inputs=max_pool_3, pool_size=3, strides=1, padding='same')\n",
    "    conv1_22 = tf.layers.conv1d(inputs=conv1_21, filters=42, kernel_size=3, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv1_32 = tf.layers.conv1d(inputs=conv1_31, filters=12, kernel_size=5, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv1_42 = tf.layers.conv1d(inputs=avg_pool1_41, filters=10, kernel_size=1, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    inception_out_1 = tf.concat([conv1_11, conv1_22, conv1_32, conv1_42], axis=2) \n",
    "    conv2_11 = tf.layers.conv1d(inputs=inception_out_1, filters=28, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv2_21 = tf.layers.conv1d(inputs=inception_out_1, filters=43, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv2_31 = tf.layers.conv1d(inputs=inception_out_1, filters=7, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    avg_pool2_41 = tf.layers.average_pooling1d(inputs=inception_out_1, pool_size=3, strides=1, padding='same')\n",
    "    conv2_22 = tf.layers.conv1d(inputs=conv2_21, filters=56, kernel_size=3, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv2_32 = tf.layers.conv1d(inputs=conv2_31, filters=14, kernel_size=5, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv2_42 = tf.layers.conv1d(inputs=avg_pool2_41, filters=14, kernel_size=1, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    inception_out_2 = tf.concat([conv2_11, conv2_22, conv2_32, conv2_42], axis=2) \n",
    "    conv3_11 = tf.layers.conv1d(inputs=inception_out_2, filters=37, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv3_21 = tf.layers.conv1d(inputs=inception_out_2, filters=56, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv3_31 = tf.layers.conv1d(inputs=inception_out_2, filters=9, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    avg_pool3_41 = tf.layers.average_pooling1d(inputs=inception_out_2, pool_size=3, strides=1, padding='same')\n",
    "    conv3_22 = tf.layers.conv1d(inputs=conv3_21, filters=73, kernel_size=3, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv3_32 = tf.layers.conv1d(inputs=conv3_31, filters=18, kernel_size=5, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv3_42 = tf.layers.conv1d(inputs=avg_pool3_41, filters=19, kernel_size=1, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    inception_out_3 = tf.concat([conv3_11, conv3_22, conv3_32, conv3_42], axis=2)\n",
    "    flat = tf.reshape(inception_out_3, (-1, 126*147))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    logits = tf.layers.dense(flat, 3700, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits= tf.nn.dropout(logits, keep_prob=keep_prob_)\n",
    "    logits_ = tf.layers.dense(logits, 740, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits_= tf.nn.dropout(logits_, keep_prob=keep_prob_)\n",
    "    logits_1 = tf.layers.dense(logits_, n_classes*5, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\"\"\"    \n",
    "    #   model architecture(CNN_6I)\n",
    "\"\"\"\n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=64, kernel_size=20, strides=2,padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=3, padding='same')\n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=15, strides=1, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=3, padding='same')\n",
    "    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=64, kernel_size=10, strides=1, padding='same', kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=1, strides=2, padding='same')\n",
    "    conv1_11 = tf.layers.conv1d(inputs=max_pool_3, filters=22, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv1_21 = tf.layers.conv1d(inputs=max_pool_3, filters=32, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv1_31 = tf.layers.conv1d(inputs=max_pool_3, filters=6, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    avg_pool1_41 = tf.layers.average_pooling1d(inputs=max_pool_3, pool_size=3, strides=1, padding='same')\n",
    "    conv1_22 = tf.layers.conv1d(inputs=conv1_21, filters=42, kernel_size=3, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv1_32 = tf.layers.conv1d(inputs=conv1_31, filters=12, kernel_size=5, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv1_42 = tf.layers.conv1d(inputs=avg_pool1_41, filters=10, kernel_size=1, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    inception_out_1 = tf.concat([conv1_11, conv1_22, conv1_32, conv1_42], axis=2) \n",
    "    conv2_11 = tf.layers.conv1d(inputs=inception_out_1, filters=28, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv2_21 = tf.layers.conv1d(inputs=inception_out_1, filters=43, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv2_31 = tf.layers.conv1d(inputs=inception_out_1, filters=7, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    avg_pool2_41 = tf.layers.average_pooling1d(inputs=inception_out_1, pool_size=3, strides=1, padding='same')\n",
    "    conv2_22 = tf.layers.conv1d(inputs=conv2_21, filters=56, kernel_size=3, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv2_32 = tf.layers.conv1d(inputs=conv2_31, filters=14, kernel_size=5, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv2_42 = tf.layers.conv1d(inputs=avg_pool2_41, filters=14, kernel_size=1, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    inception_out_2 = tf.concat([conv2_11, conv2_22, conv2_32, conv2_42], axis=2) \n",
    "    conv3_11 = tf.layers.conv1d(inputs=inception_out_2, filters=37, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv3_21 = tf.layers.conv1d(inputs=inception_out_2, filters=56, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv3_31 = tf.layers.conv1d(inputs=inception_out_2, filters=9, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    avg_pool3_41 = tf.layers.average_pooling1d(inputs=inception_out_2, pool_size=3, strides=1, padding='same')\n",
    "    conv3_22 = tf.layers.conv1d(inputs=conv3_21, filters=73, kernel_size=3, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv3_32 = tf.layers.conv1d(inputs=conv3_31, filters=18, kernel_size=5, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv3_42 = tf.layers.conv1d(inputs=avg_pool3_41, filters=19, kernel_size=1, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    inception_out_3 = tf.concat([conv3_11, conv3_22, conv3_32, conv3_42], axis=2)\n",
    "    conv4_11 = tf.layers.conv1d(inputs=inception_out_3, filters=49, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv4_21 = tf.layers.conv1d(inputs=inception_out_3, filters=74, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv4_31 = tf.layers.conv1d(inputs=inception_out_3, filters=12, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    avg_pool4_41 = tf.layers.average_pooling1d(inputs=inception_out_3, pool_size=3, strides=1, padding='same')\n",
    "    conv4_22 = tf.layers.conv1d(inputs=conv4_21, filters=96, kernel_size=3, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv4_32 = tf.layers.conv1d(inputs=conv4_31, filters=24, kernel_size=5, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv4_42 = tf.layers.conv1d(inputs=avg_pool4_41, filters=25, kernel_size=1, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    inception_out_4 = tf.concat([conv4_11, conv4_22, conv4_32, conv4_42], axis=2)\n",
    "    conv5_11 = tf.layers.conv1d(inputs=inception_out_4, filters=65, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv5_21 = tf.layers.conv1d(inputs=inception_out_4, filters=97, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv5_31 = tf.layers.conv1d(inputs=inception_out_4, filters=16, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    avg_pool5_41 = tf.layers.average_pooling1d(inputs=inception_out_4, pool_size=3, strides=1, padding='same')\n",
    "    conv5_22 = tf.layers.conv1d(inputs=conv4_21, filters=126, kernel_size=3, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv5_32 = tf.layers.conv1d(inputs=conv4_31, filters=32, kernel_size=5, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv5_42 = tf.layers.conv1d(inputs=avg_pool4_41, filters=32, kernel_size=1, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    inception_out_5 = tf.concat([conv5_11, conv5_22, conv5_32, conv5_42], axis=2)\n",
    "    conv6_11 = tf.layers.conv1d(inputs=inception_out_5, filters=85, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv6_21 = tf.layers.conv1d(inputs=inception_out_5, filters=128, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    conv6_31 = tf.layers.conv1d(inputs=inception_out_5, filters=19, kernel_size=1, strides=1, padding='same', activation = tf.nn.relu)\n",
    "    avg_pool6_41 = tf.layers.average_pooling1d(inputs=inception_out_5, pool_size=3, strides=1, padding='same')\n",
    "    conv6_22 = tf.layers.conv1d(inputs=conv6_21, filters=166, kernel_size=3, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv6_32 = tf.layers.conv1d(inputs=conv6_31, filters=38, kernel_size=5, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    conv6_42 = tf.layers.conv1d(inputs=avg_pool6_41, filters=43, kernel_size=1, strides=1, padding='same', activation=tf.nn.relu)\n",
    "    inception_out_6 = tf.concat([conv6_11, conv6_22, conv6_32, conv6_42], axis=2)\n",
    "    flat = tf.reshape(inception_out_6, (-1, 126*332))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    logits = tf.layers.dense(flat, 4000, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits= tf.nn.dropout(logits, keep_prob=keep_prob_)\n",
    "    logits_ = tf.layers.dense(logits, 400, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits_= tf.nn.dropout(logits_, keep_prob=keep_prob_)\n",
    "    logits_1 = tf.layers.dense(logits_, n_classes*5, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\"\"\"\"    \n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_1,labels=labels_1)) \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost)\n",
    "    correct_pred = tf.equal(logit_num, label_num)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_acc = []\n",
    "validation_loss = []\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('file path'))\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        for i in  range(600):\n",
    "            X_tr, y_tr, y_tr_p, y_ind = sess.run([X_train, y_train, y_train_p, y_train_ind])\n",
    "            X_tr= np.reshape(X_tr, (-1, 4501, 1))\n",
    "            feed = {inputs_ : X_tr, labels_1 : y_tr, labels_2 : y_tr_p, keep_prob_ : 0.5, learning_rate_ : learning_rate}\n",
    "            loss, _ , logit = sess.run([cost, optimizer, logits_1], feed_dict = feed)            \n",
    "            y_lab = np.empty([batch_size_, 3])\n",
    "            y_logit = np.empty([batch_size_, 3])\n",
    "            for i in range(batch_size_):\n",
    "                if y_ind[i,0] == 2:\n",
    "                    y_lab[i]=np.argsort(y_tr[i])[-3:]\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    y_logit[i]=np.argsort(logit[i])[-3:]\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "                elif y_ind[i,0] == 1:\n",
    "                    z=np.argsort(y_tr[i])[-2:]\n",
    "                    y_lab[i]=np.append(z, [0])\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    z_=np.argsort(logit[i])[-2:]\n",
    "                    y_logit[i]=np.append(z_, [0])\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "                elif y_ind[i,0] == 0:\n",
    "                    z=np.argsort(y_tr[i])[-1:]\n",
    "                    y_lab[i]=np.append(z, [0,0])\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    z_=np.argsort(logit[i])[-1:]\n",
    "                    y_logit[i]=np.append(z_, [0,0])\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "                else:\n",
    "                    print('Something Wrong happened!!!')       \n",
    "            feed = {logit_num : y_logit, label_num: y_lab}\n",
    "            acc = sess.run(accuracy, feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                    \"Iteration: {:d}\".format(iteration),\n",
    "                    \"Train loss: {:6f}\".format(loss),\n",
    "                    \"Train acc: {:.6f}\".format(acc))\n",
    "\n",
    "###================================================================ VALIDATION =====================================\n",
    "            if (iteration %5 == 0):\n",
    "                X_vd, y_vd, y_vd_p, y_ind_vd = sess.run([X_vld, y_vld, y_vld_p, y_vld_ind])\n",
    "                X_vd= np.reshape(X_vd, (-1, 4501, 1))\n",
    "                feed = {inputs_ : X_vd, labels_1 : y_vd, labels_2 : y_vd_p, keep_prob_ : 1.0, learning_rate_ : learning_rate}\n",
    "                loss_vd,  logit_vd = sess.run([cost,  logits_1], feed_dict = feed)            \n",
    "                y_lab_vd = np.empty([batch_size_, 3])\n",
    "                y_logit_vd = np.empty([batch_size_, 3])\n",
    "                for i in range(batch_size_):\n",
    "                    if y_ind_vd[i,0] == 2:\n",
    "                        y_lab_vd[i]=np.argsort(y_vd[i])[-3:]\n",
    "                        y_lab_vd[i]=np.sort(y_lab_vd[i])\n",
    "                        y_logit_vd[i]=np.argsort(logit_vd[i])[-3:]\n",
    "                        y_logit_vd[i]=np.sort(y_logit_vd[i])\n",
    "                    elif y_ind_vd[i,0] == 1:\n",
    "                        z=np.argsort(y_vd[i])[-2:]\n",
    "                        y_lab_vd[i]=np.append(z, [0])\n",
    "                        y_lab_vd[i]=np.sort(y_lab_vd[i])\n",
    "                        z_=np.argsort(logit_vd[i])[-2:]\n",
    "                        y_logit_vd[i]=np.append(z_, [0])\n",
    "                        y_logit_vd[i]=np.sort(y_logit_vd[i])\n",
    "                    elif y_ind_vd[i,0] == 0:\n",
    "                        z=np.argsort(y_vd[i])[-1:]\n",
    "                        y_lab_vd[i]=np.append(z, [0,0])\n",
    "                        y_lab_vd[i]=np.sort(y_lab_vd[i])\n",
    "                        z_=np.argsort(logit_vd[i])[-1:]\n",
    "                        y_logit_vd[i]=np.append(z_, [0,0])\n",
    "                        y_logit_vd[i]=np.sort(y_logit_vd[i])\n",
    "                    else:\n",
    "                        print('Something Wrong happened!!!')\n",
    "                feed_1 = {logit_num : y_logit_vd, label_num: y_lab_vd}\n",
    "                acc_vd = sess.run(accuracy, feed_dict = feed_1)\n",
    "                validation_acc.append(acc_vd)\n",
    "                validation_loss.append(loss_vd)\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                        \"Iteration: {:d}\".format(iteration),\n",
    "                        \"Validation loss: {:6f}\".format(loss_vd),\n",
    "                        \"Validation acc: {:.6f}\".format(acc_vd))\n",
    "            iteration += 1 \n",
    "        saver.save(sess,'file path')\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
